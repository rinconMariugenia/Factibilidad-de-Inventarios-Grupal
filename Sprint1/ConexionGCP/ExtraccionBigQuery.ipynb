{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NPI5PsgffJg"
   },
   "source": [
    "## Proyecto Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3QRoWE9ffJh"
   },
   "source": [
    "pip install pandas as pd\n",
    "pip install numpy as np\n",
    "pip install pandas pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mta7xA71TpuK"
   },
   "outputs": [],
   "source": [
    "# Carga de librerias\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisión archivo 2017PurchasePicesDec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de filas: 12261\n",
      "Total de columnas: 9\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Ruta al archivo JSON de credenciales\n",
    "key_path = 'elixiemporium-0b9fa2f9c940.json'\n",
    "\n",
    "# Inicializar el cliente de BigQuery\n",
    "client = bigquery.Client.from_service_account_json(key_path)\n",
    "\n",
    "# Especificar la tabla\n",
    "table_id = 'elixiemporium.elixiremporiumdatossinETL.2017PurchasePricesDec'\n",
    "\n",
    "# Obtener el número de filas\n",
    "query_job = client.query(f\"SELECT COUNT(*) AS total_filas FROM `{table_id}`\")\n",
    "results = query_job.result()\n",
    "for row in results:\n",
    "    total_filas = row.total_filas\n",
    "    print(f\"Total de filas: {total_filas}\")\n",
    "\n",
    "# Obtener el número de columnas (utilizando el esquema de la tabla)\n",
    "table = client.get_table(table_id)\n",
    "total_columnas = len(table.schema)\n",
    "print(f\"Total de columnas: {total_columnas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: db-dtypes in c:\\users\\cami\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\cami\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from db-dtypes) (24.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\cami\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from db-dtypes) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=3.0.0 in c:\\users\\cami\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from db-dtypes) (19.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\cami\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from db-dtypes) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\cami\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=0.24.2->db-dtypes) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cami\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=0.24.2->db-dtypes) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\cami\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=0.24.2->db-dtypes) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cami\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.24.2->db-dtypes) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install db-dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cami\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:1820: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Column  Null_Count\n",
      "0           Brand_nulls           0\n",
      "1     Description_nulls           0\n",
      "2           Price_nulls           0\n",
      "3            Size_nulls           0\n",
      "4          Volume_nulls           0\n",
      "5  Classification_nulls           0\n",
      "6   PurchasePrice_nulls           0\n",
      "7    VendorNumber_nulls           0\n",
      "8      VendorName_nulls           0\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "# Autenticación con el archivo JSON\n",
    "client = bigquery.Client.from_service_account_json(\"elixiemporium-0b9fa2f9c940.json\")\n",
    "\n",
    "# Definir el proyecto, dataset y tabla\n",
    "project_id = \"elixiemporium\"\n",
    "dataset_id = \"elixiremporiumdatossinETL\"\n",
    "table_id = \"2017PurchasePricesDec\"\n",
    "table_full_name = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# Obtener el esquema de la tabla para listar las columnas\n",
    "table = client.get_table(table_full_name)\n",
    "columns = [schema_field.name for schema_field in table.schema]\n",
    "\n",
    "# Construir la consulta SQL dinámicamente\n",
    "null_counts_query = \"SELECT \" + \", \".join(\n",
    "    [f\"SUM(CASE WHEN `{col}` IS NULL THEN 1 ELSE 0 END) AS `{col}_nulls`\" for col in columns]\n",
    ") + f\" FROM `{table_full_name}`\"\n",
    "\n",
    "# Ejecutar la consulta\n",
    "query_job = client.query(null_counts_query)\n",
    "df = query_job.to_dataframe()\n",
    "\n",
    "# Transponer el DataFrame para mostrar los valores por columna\n",
    "df_transposed = df.T.reset_index()\n",
    "df_transposed.columns = [\"Column\", \"Null_Count\"]\n",
    "df_transposed = df_transposed.sort_values(by=\"Null_Count\", ascending=False)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(df_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequest",
     "evalue": "400 No matching signature for aggregate function SUM\n  Argument types: STRING\n  Signature: SUM(INT64)\n    Argument 1: Unable to coerce type STRING to expected type INT64\n  Signature: SUM(FLOAT64)\n    Argument 1: Unable to coerce type STRING to expected type FLOAT64\n  Signature: SUM(NUMERIC)\n    Argument 1: Unable to coerce type STRING to expected type NUMERIC\n  Signature: SUM(BIGNUMERIC)\n    Argument 1: Unable to coerce type STRING to expected type BIGNUMERIC\n  Signature: SUM(INTERVAL)\n    Argument 1: Unable to coerce type STRING to expected type INTERVAL at [1:8]; reason: invalidQuery, location: query, message: No matching signature for aggregate function SUM\n  Argument types: STRING\n  Signature: SUM(INT64)\n    Argument 1: Unable to coerce type STRING to expected type INT64\n  Signature: SUM(FLOAT64)\n    Argument 1: Unable to coerce type STRING to expected type FLOAT64\n  Signature: SUM(NUMERIC)\n    Argument 1: Unable to coerce type STRING to expected type NUMERIC\n  Signature: SUM(BIGNUMERIC)\n    Argument 1: Unable to coerce type STRING to expected type BIGNUMERIC\n  Signature: SUM(INTERVAL)\n    Argument 1: Unable to coerce type STRING to expected type INTERVAL at [1:8]\n\nLocation: US\nJob ID: b3b2da48-874a-4a26-a8d3-3b7b3a6b5a56\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Ejecutar la consulta\u001b[39;00m\n\u001b[0;32m     23\u001b[0m query_job \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mquery(null_counts_query)\n\u001b[1;32m---> 24\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mquery_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Transponer el DataFrame (optional)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# This step is optional if you prefer the output in row format\u001b[39;00m\n\u001b[0;32m     28\u001b[0m df_transposed \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32mc:\\Users\\Cami\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:2057\u001b[0m, in \u001b[0;36mQueryJob.to_dataframe\u001b[1;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dataframe\u001b[39m(\n\u001b[0;32m   1828\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1829\u001b[0m     bqstorage_client: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1849\u001b[0m     ] \u001b[38;5;241m=\u001b[39m DefaultPandasDTypes\u001b[38;5;241m.\u001b[39mRANGE_TIMESTAMP_DTYPE,\n\u001b[0;32m   1850\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas.DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1851\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[0;32m   1852\u001b[0m \n\u001b[0;32m   1853\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2055\u001b[0m \u001b[38;5;124;03m            :mod:`shapely` library cannot be imported.\u001b[39;00m\n\u001b[0;32m   2056\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2057\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[43mwait_for_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m query_result\u001b[38;5;241m.\u001b[39mto_dataframe(\n\u001b[0;32m   2059\u001b[0m         bqstorage_client\u001b[38;5;241m=\u001b[39mbqstorage_client,\n\u001b[0;32m   2060\u001b[0m         dtypes\u001b[38;5;241m=\u001b[39mdtypes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2074\u001b[0m         range_timestamp_dtype\u001b[38;5;241m=\u001b[39mrange_timestamp_dtype,\n\u001b[0;32m   2075\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Cami\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\cloud\\bigquery\\_tqdm_helpers.py:107\u001b[0m, in \u001b[0;36mwait_for_query\u001b[1;34m(query_job, progress_bar_type, max_results)\u001b[0m\n\u001b[0;32m    103\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m get_progress_bar(\n\u001b[0;32m    104\u001b[0m     progress_bar_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery is running\u001b[39m\u001b[38;5;124m\"\u001b[39m, default_total, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Cami\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1681\u001b[0m, in \u001b[0;36mQueryJob.result\u001b[1;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;66;03m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[39;00m\n\u001b[0;32m   1680\u001b[0m     \u001b[38;5;66;03m# long-running API, don't delay the next request at all.\u001b[39;00m\n\u001b[1;32m-> 1681\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_job_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1682\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1684\u001b[0m     \u001b[38;5;66;03m# Use a monotonic clock since we don't actually care about\u001b[39;00m\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# daylight savings or similar, just the elapsed time.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Cami\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Cami\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[1;32mc:\\Users\\Cami\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32mc:\\Users\\Cami\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\Cami\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\cloud\\bigquery\\job\\query.py:1630\u001b[0m, in \u001b[0;36mQueryJob.result.<locals>.is_job_done\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job_failed_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1608\u001b[0m     \u001b[38;5;66;03m# Only try to restart the query job if the job failed for\u001b[39;00m\n\u001b[0;32m   1609\u001b[0m     \u001b[38;5;66;03m# a retriable reason. For example, don't restart the query\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1627\u001b[0m     \u001b[38;5;66;03m# into an exception that can be processed by the\u001b[39;00m\n\u001b[0;32m   1628\u001b[0m     \u001b[38;5;66;03m# `job_retry` predicate.\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m     restart_query_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m job_failed_exception\n\u001b[0;32m   1631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1632\u001b[0m     \u001b[38;5;66;03m# Make sure that the _query_results are cached so we\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m     \u001b[38;5;66;03m# can return a complete RowIterator.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1639\u001b[0m     \u001b[38;5;66;03m# making any extra API calls if the previous loop\u001b[39;00m\n\u001b[0;32m   1640\u001b[0m     \u001b[38;5;66;03m# iteration fetched the finished job.\u001b[39;00m\n\u001b[0;32m   1641\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reload_query_results(\n\u001b[0;32m   1642\u001b[0m         retry\u001b[38;5;241m=\u001b[39mretry, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreload_query_results_kwargs\n\u001b[0;32m   1643\u001b[0m     )\n",
      "\u001b[1;31mBadRequest\u001b[0m: 400 No matching signature for aggregate function SUM\n  Argument types: STRING\n  Signature: SUM(INT64)\n    Argument 1: Unable to coerce type STRING to expected type INT64\n  Signature: SUM(FLOAT64)\n    Argument 1: Unable to coerce type STRING to expected type FLOAT64\n  Signature: SUM(NUMERIC)\n    Argument 1: Unable to coerce type STRING to expected type NUMERIC\n  Signature: SUM(BIGNUMERIC)\n    Argument 1: Unable to coerce type STRING to expected type BIGNUMERIC\n  Signature: SUM(INTERVAL)\n    Argument 1: Unable to coerce type STRING to expected type INTERVAL at [1:8]; reason: invalidQuery, location: query, message: No matching signature for aggregate function SUM\n  Argument types: STRING\n  Signature: SUM(INT64)\n    Argument 1: Unable to coerce type STRING to expected type INT64\n  Signature: SUM(FLOAT64)\n    Argument 1: Unable to coerce type STRING to expected type FLOAT64\n  Signature: SUM(NUMERIC)\n    Argument 1: Unable to coerce type STRING to expected type NUMERIC\n  Signature: SUM(BIGNUMERIC)\n    Argument 1: Unable to coerce type STRING to expected type BIGNUMERIC\n  Signature: SUM(INTERVAL)\n    Argument 1: Unable to coerce type STRING to expected type INTERVAL at [1:8]\n\nLocation: US\nJob ID: b3b2da48-874a-4a26-a8d3-3b7b3a6b5a56\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "# Autenticación con el archivo JSON\n",
    "client = bigquery.Client.from_service_account_json(\"elixiemporium-0b9fa2f9c940.json\")\n",
    "\n",
    "# Definir el proyecto, dataset y tabla\n",
    "project_id = \"elixiemporium\"\n",
    "dataset_id = \"elixiremporiumdatossinETL\"\n",
    "table_id = \"2017PurchasePricesDec\"\n",
    "table_full_name = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# Obtener el esquema de la tabla para listar las columnas\n",
    "table = client.get_table(table_full_name)\n",
    "columns = [schema_field.name for schema_field in table.schema]\n",
    "\n",
    "# Construir la consulta SQL dinámicamente\n",
    "null_counts_query = \"SELECT \" + \", \".join(\n",
    "    [f\"SUM(CASE WHEN `{col}` IS NULL THEN 'NaN' ELSE CAST({col} AS STRING) END) AS `{col}`\" for col in columns]\n",
    ") + f\" FROM `{table_full_name}`\"\n",
    "\n",
    "# Ejecutar la consulta\n",
    "query_job = client.query(null_counts_query)\n",
    "df = query_job.to_dataframe()\n",
    "\n",
    "# Transponer el DataFrame (optional)\n",
    "# This step is optional if you prefer the output in row format\n",
    "df_transposed = df.T.reset_index()\n",
    "df_transposed.columns = [\"Column\", \"Value\"]\n",
    "df_transposed = df_transposed.sort_values(by=\"Column\")  # Sort by column name\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(df_transposed if len(df_transposed) > 0 else \"No data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'column' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m\n\u001b[0;32m     18\u001b[0m columns \u001b[38;5;241m=\u001b[39m [schema_field\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m schema_field \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mschema]\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Construir la consulta SQL dinámicamente\u001b[39;00m\n\u001b[0;32m     21\u001b[0m null_counts_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;124m    SUM(CASE WHEN \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcolumn\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m IS NULL THEN 1 ELSE 0 END) AS total_nulls\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124mFROM \u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124m    `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_full_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Ejecutar la consulta\u001b[39;00m\n\u001b[0;32m     29\u001b[0m query_job \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mquery(null_counts_query)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'column' is not defined"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo JSON de credenciales\n",
    "key_path = 'elixiemporium-0b9fa2f9c940.json'\n",
    "\n",
    "# Inicializar el cliente de BigQuery\n",
    "client = bigquery.Client.from_service_account_json(key_path)\n",
    "\n",
    "# Especificar la tabla\n",
    "project_id = \"elixiemporium\"\n",
    "dataset_id = \"elixiremporiumdatossinETL\"\n",
    "table_id = \"2017PurchasePricesDec\"\n",
    "table_full_name = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# Obtener el esquema de la tabla para listar las columnas\n",
    "table = client.get_table(table_full_name)\n",
    "columns = [schema_field.name for schema_field in table.schema]\n",
    "\n",
    "# Construir la consulta SQL dinámicamente\n",
    "null_counts_query = f\"\"\"\n",
    "SELECT \n",
    "    SUM(CASE WHEN {column} IS NULL THEN 1 ELSE 0 END) AS total_nulls\n",
    "FROM \n",
    "    `{table_full_name}`\n",
    "\"\"\"\n",
    "\n",
    "# Ejecutar la consulta\n",
    "query_job = client.query(null_counts_query)\n",
    "results = query_job.result()\n",
    "\n",
    "# Imprimir los resultados\n",
    "for row in results:\n",
    "    print(f\"Total de valores nulos: {row.total_nulls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
